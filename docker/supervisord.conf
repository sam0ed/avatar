; Avatar Stage 2 â€” supervisord program definitions
; Manages 3 services in a single container:
;   1. LLM server  (llama-cpp-python, port 8001)
;   2. TTS server  (Fish Speech S1-mini, port 8080)
;   3. Orchestrator (FastAPI WebSocket, port 8000)
;
; Included by /etc/supervisor/supervisord.conf via [include] directive.
; Run with: supervisord -n -c /etc/supervisor/supervisord.conf

[program:llm]
command=/opt/llm-venv/bin/python -m llama_cpp.server
    --model %(ENV_MODEL_DIR)s/%(ENV_MODEL_FILE)s
    --host 0.0.0.0
    --port %(ENV_LLM_PORT)s
    --n_gpu_layers %(ENV_N_GPU_LAYERS)s
    --n_ctx %(ENV_N_CTX)s
    --flash_attn %(ENV_FLASH_ATTN)s
directory=/app
priority=10
autostart=true
autorestart=true
startretries=3
startsecs=10
stdout_logfile=/var/log/supervisor/llm.log
stderr_logfile=/var/log/supervisor/llm_err.log
stdout_logfile_maxbytes=10MB
stderr_logfile_maxbytes=10MB

[program:tts]
command=bash start_server.sh
directory=/app
priority=20
autostart=true
autorestart=true
startretries=3
startsecs=10
stdout_logfile=/var/log/supervisor/tts.log
stderr_logfile=/var/log/supervisor/tts_err.log
stdout_logfile_maxbytes=10MB
stderr_logfile_maxbytes=10MB

[program:orchestrator]
command=uv run uvicorn src.api.server:app --host 0.0.0.0 --port 8000
directory=/app/orchestrator
environment=LLM_BASE_URL="http://localhost:8001",TTS_BASE_URL="http://localhost:8080"
priority=30
autostart=true
autorestart=true
startretries=5
startsecs=5
stdout_logfile=/var/log/supervisor/orchestrator.log
stderr_logfile=/var/log/supervisor/orchestrator_err.log
stdout_logfile_maxbytes=10MB
stderr_logfile_maxbytes=10MB
